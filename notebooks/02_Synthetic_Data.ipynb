{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Synthetic Data Generation\n",
                "\n",
                "This notebook standardized imputed data and then generates synthetic data using CTGAN, TVAE, and ADASYN for each of the imputed datasets (MICE, KNN, SoftImpute, GAIN).\n",
                "Results are saved in `data/synthetic/{imputation_method}/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer\n",
                "from sdv.metadata import SingleTableMetadata\n",
                "from imblearn.over_sampling import ADASYN, SMOTE\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import os\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "imputation_methods = ['mice', 'knn', 'softimpute', 'gain', 'pmm']\n",
                "target_col = 'mayo' # Target column for ADASYN oversampling\n",
                "\n",
                "# Ensure base directory exists\n",
                "os.makedirs('../data/synthetic', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "generation_loop",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "Processing Imputation Method: MICE\n",
                        "========================================\n",
                        "Original data loaded: (252, 56)\n",
                        "  > Standardizing data...\n",
                        "    Data standardized.\n",
                        "  > Generating CTGAN for mice (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Gen. (-4.46) | Discrim. (0.01): 100%|██████████| 300/300 [00:37<00:00,  8.07it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    CTGAN Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating TVAE for mice (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: -64.783: 100%|██████████| 300/300 [00:14<00:00, 21.23it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    TVAE Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating ADASYN for mice (Oversampling)...\n",
                        "    ADASYN Saved. Final shape: (340, 56)\n",
                        "  > Generating SMOTE for mice...\n",
                        "    SMOTE Saved. New shape: (352, 56)\n",
                        "\n",
                        "========================================\n",
                        "Processing Imputation Method: KNN\n",
                        "========================================\n",
                        "Original data loaded: (252, 56)\n",
                        "  > Standardizing data...\n",
                        "    Data standardized.\n",
                        "  > Generating CTGAN for knn (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Gen. (-3.32) | Discrim. (0.01): 100%|██████████| 300/300 [00:37<00:00,  8.07it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    CTGAN Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating TVAE for knn (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: -53.084: 100%|██████████| 300/300 [00:14<00:00, 21.28it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    TVAE Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating ADASYN for knn (Oversampling)...\n",
                        "    ADASYN Saved. Final shape: (351, 56)\n",
                        "  > Generating SMOTE for knn...\n",
                        "    SMOTE Saved. New shape: (352, 56)\n",
                        "\n",
                        "========================================\n",
                        "Processing Imputation Method: SOFTIMPUTE\n",
                        "========================================\n",
                        "Original data loaded: (252, 56)\n",
                        "  > Standardizing data...\n",
                        "    Data standardized.\n",
                        "  > Generating CTGAN for softimpute (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Gen. (-5.04) | Discrim. (0.08): 100%|██████████| 300/300 [00:37<00:00,  8.06it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    CTGAN Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating TVAE for softimpute (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: -62.355: 100%|██████████| 300/300 [00:14<00:00, 21.06it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    TVAE Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating ADASYN for softimpute (Oversampling)...\n",
                        "    ADASYN Saved. Final shape: (346, 56)\n",
                        "  > Generating SMOTE for softimpute...\n",
                        "    SMOTE Saved. New shape: (352, 56)\n",
                        "\n",
                        "========================================\n",
                        "Processing Imputation Method: GAIN\n",
                        "========================================\n",
                        "Original data loaded: (252, 56)\n",
                        "  > Standardizing data...\n",
                        "    Data standardized.\n",
                        "  > Generating CTGAN for gain (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Gen. (-4.73) | Discrim. (0.34): 100%|██████████| 300/300 [00:37<00:00,  7.99it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    CTGAN Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating TVAE for gain (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: -51.664: 100%|██████████| 300/300 [00:14<00:00, 20.47it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    TVAE Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating ADASYN for gain (Oversampling)...\n",
                        "    ADASYN Saved. Final shape: (351, 56)\n",
                        "  > Generating SMOTE for gain...\n",
                        "    SMOTE Saved. New shape: (352, 56)\n",
                        "\n",
                        "========================================\n",
                        "Processing Imputation Method: PMM\n",
                        "========================================\n",
                        "Original data loaded: (252, 56)\n",
                        "  > Standardizing data...\n",
                        "    Data standardized.\n",
                        "  > Generating CTGAN for pmm (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Gen. (-4.09) | Discrim. (0.15): 100%|██████████| 300/300 [00:37<00:00,  8.00it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    CTGAN Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating TVAE for pmm (Augmentation)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: -43.744: 100%|██████████| 300/300 [00:14<00:00, 21.07it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    TVAE Augmented Saved. Final shape: (504, 56)\n",
                        "  > Generating ADASYN for pmm (Oversampling)...\n",
                        "    ADASYN Saved. Final shape: (347, 56)\n",
                        "  > Generating SMOTE for pmm...\n",
                        "    SMOTE Saved. New shape: (352, 56)\n"
                    ]
                }
            ],
            "source": [
                "for method in imputation_methods:\n",
                "    print(f\"\\n{'='*40}\\nProcessing Imputation Method: {method.upper()}\\n{'='*40}\")\n",
                "    \n",
                "    input_path = f'../data/processed/uc_diagnostic_tests_{method}.csv'\n",
                "    if not os.path.exists(input_path):\n",
                "        print(f\"File not found: {input_path} -- Skipping.\")\n",
                "        continue\n",
                "        \n",
                "    df_original = pd.read_csv(input_path) \n",
                "    print(f\"Original data loaded: {df_original.shape}\")\n",
                "    \n",
                "    # --- STANDARDIZATION ---\n",
                "    print(\"  > Standardizing data...\")\n",
                "    if target_col in df_original.columns:\n",
                "        X = df_original.drop(columns=[target_col])\n",
                "        y = df_original[target_col]\n",
                "    else:\n",
                "        X = df_original\n",
                "        y = None\n",
                "        \n",
                "    scaler = StandardScaler()\n",
                "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
                "    \n",
                "    if y is not None:\n",
                "        X_scaled.reset_index(drop=True, inplace=True)\n",
                "        y.reset_index(drop=True, inplace=True)\n",
                "        df_scaled = pd.concat([X_scaled, y], axis=1)\n",
                "    else:\n",
                "        df_scaled = X_scaled\n",
                "\n",
                "    print(\"    Data standardized.\")\n",
                "    \n",
                "    output_dir = f'../data/synthetic/{method}'\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "    metadata = SingleTableMetadata()\n",
                "    metadata.detect_from_dataframe(data=df_scaled)\n",
                "    \n",
                "    # Helper function to inverse transform back to original scale\n",
                "    def inverse_transform_data(synthetic_data, scaler, target_col):\n",
                "        if target_col in synthetic_data.columns:\n",
                "            X_syn = synthetic_data.drop(columns=[target_col])\n",
                "            y_syn = synthetic_data[target_col]\n",
                "        else:\n",
                "            X_syn = synthetic_data\n",
                "            y_syn = None\n",
                "            \n",
                "        X_syn_inv = pd.DataFrame(scaler.inverse_transform(X_syn), columns=X_syn.columns)\n",
                "        \n",
                "        if y_syn is not None:\n",
                "            X_syn_inv.reset_index(drop=True, inplace=True)\n",
                "            y_syn.reset_index(drop=True, inplace=True)\n",
                "            df_syn_inv = pd.concat([X_syn_inv, y_syn], axis=1)\n",
                "        else:\n",
                "            df_syn_inv = X_syn_inv\n",
                "        return df_syn_inv\n",
                "    \n",
                "    # --- CTGAN ---\n",
                "    print(f\"  > Generating CTGAN for {method} (Augmentation)...\")\n",
                "    try:\n",
                "        ctgan = CTGANSynthesizer(metadata, epochs=300, verbose=True)\n",
                "        ctgan.fit(df_scaled)\n",
                "        # Generate 252 new records\n",
                "        synthetic_ctgan_scaled = ctgan.sample(num_rows=len(df_original))\n",
                "        synthetic_ctgan_inv = inverse_transform_data(synthetic_ctgan_scaled, scaler, target_col)\n",
                "        \n",
                "        # COMBINE: Original + Synthetic\n",
                "        df_augmented_ctgan = pd.concat([df_original, synthetic_ctgan_inv], axis=0).reset_index(drop=True)\n",
                "        df_augmented_ctgan.to_csv(f'{output_dir}/uc_diagnostics_ctgan.csv', index=False)\n",
                "        print(f\"    CTGAN Augmented Saved. Final shape: {df_augmented_ctgan.shape}\")\n",
                "    except Exception as e:\n",
                "        print(f\"    CTGAN Failed: {e}\")\n",
                "    \n",
                "    # --- TVAE ---\n",
                "    print(f\"  > Generating TVAE for {method} (Augmentation)...\")\n",
                "    try:\n",
                "        tvae = TVAESynthesizer(metadata, epochs=300, verbose=True)\n",
                "        tvae.fit(df_scaled)\n",
                "        # Generate 252 new records\n",
                "        synthetic_tvae_scaled = tvae.sample(num_rows=len(df_original))\n",
                "        synthetic_tvae_inv = inverse_transform_data(synthetic_tvae_scaled, scaler, target_col)\n",
                "        \n",
                "        # COMBINE: Original + Synthetic\n",
                "        df_augmented_tvae = pd.concat([df_original, synthetic_tvae_inv], axis=0).reset_index(drop=True)\n",
                "        df_augmented_tvae.to_csv(f'{output_dir}/uc_diagnostics_tvae.csv', index=False)\n",
                "        print(f\"    TVAE Augmented Saved. Final shape: {df_augmented_tvae.shape}\")\n",
                "    except Exception as e:\n",
                "        print(f\"    TVAE Failed: {e}\")\n",
                "    \n",
                "    # --- ADASYN ---\n",
                "    print(f\"  > Generating ADASYN for {method} (Oversampling)...\")\n",
                "    try:\n",
                "        if y is None or y.isnull().any():\n",
                "             print(\"    ADASYN Skipping: Target column missing or contains NaNs.\")\n",
                "        else:\n",
                "            # ADASYN on Scaled X and original y\n",
                "            adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
                "            X_res, y_res = adasyn.fit_resample(X_scaled, y)\n",
                "            \n",
                "            # Inverse transform the combined (resampled) dataset\n",
                "            X_res_inv = pd.DataFrame(scaler.inverse_transform(X_res), columns=X_res.columns)\n",
                "            df_adasyn_inv = pd.concat([X_res_inv, y_res.reset_index(drop=True)], axis=1)\n",
                "            \n",
                "            df_adasyn_inv.to_csv(f'{output_dir}/uc_diagnostics_adasyn.csv', index=False)\n",
                "            print(f\"    ADASYN Saved. Final shape: {df_adasyn_inv.shape}\")\n",
                "    except Exception as e:\n",
                "        print(f\"    ADASYN Failed: {e}\")\n",
                "\n",
                "    # --- SMOTE ---\n",
                "    print(f\"  > Generating SMOTE for {method}...\")\n",
                "    try:\n",
                "        from imblearn.over_sampling import SMOTE\n",
                "        smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
                "        X_res, y_res = smote.fit_resample(X_scaled, y)\n",
                "        \n",
                "        synthetic_smote = pd.concat([X_res, y_res], axis=1)\n",
                "        \n",
                "        inverse_transform_and_save(synthetic_smote, f'{output_dir}/uc_diagnostics_smote.csv', scaler, target_col)\n",
                "        print(f\"    SMOTE Saved. New shape: {synthetic_smote.shape}\")\n",
                "    except Exception as e:\n",
                "        print(f\"    SMOTE Failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "uc_experiment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
