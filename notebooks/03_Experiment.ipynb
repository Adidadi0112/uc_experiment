{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. Experiment - Synthetic Data Evaluation (Stratified 5x2 CV)\n",
                "\n",
                "This notebook performs a scientifically rigorous evaluation of Random Forest, CatBoost, and a Stacking Ensemble across various synthetic datasets. \n",
                "\n",
                "We use **Stratified 5x2 Cross-Validation** (Dietterich's rule) to ensure statistical stability and handle class imbalance in the `mayo` score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
                "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, make_scorer, confusion_matrix\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_theme(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Run Iteration over All Datasets\n",
                "\n",
                "We iterate through all imputation (directories) and synthesis (files) methods, applying Stratified 5x2 CV to each combination."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "synthetic_root = '../data/synthetic'\n",
                "all_results = []\n",
                "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=42)\n",
                "\n",
                "for imputation_method in os.listdir(synthetic_root):\n",
                "    imputation_path = os.path.join(synthetic_root, imputation_method)\n",
                "    if not os.path.isdir(imputation_path): continue\n",
                "    \n",
                "    for file in os.listdir(imputation_path):\n",
                "        if not file.endswith('.csv'): continue\n",
                "        \n",
                "        synthesis_method = file.replace('uc_diagnostics_', '').replace('.csv', '')\n",
                "        file_path = os.path.join(imputation_path, file)\n",
                "        \n",
                "        # Load data\n",
                "        df = pd.read_csv(file_path)\n",
                "        \n",
                "        # Prepare features and target\n",
                "        X = df.drop(columns=['mayo'])\n",
                "        y = df['mayo']\n",
                "        \n",
                "        print(f\"Processing: Imputation={imputation_method}, Synthesis={synthesis_method}...\")\n",
                "        \n",
                "        # Define models\n",
                "        models = {\n",
                "            'RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "            'CatBoost': CatBoostClassifier(iterations=500, random_seed=42, verbose=0),\n",
                "            'Stacking': StackingClassifier(\n",
                "                estimators=[\n",
                "                    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "                    ('cb', CatBoostClassifier(iterations=500, random_seed=42, verbose=0))\n",
                "                ], \n",
                "                final_estimator=LogisticRegression()\n",
                "            )\n",
                "        }\n",
                "        \n",
                "        scoring = {\n",
                "            'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
                "            'accuracy': make_scorer(accuracy_score),\n",
                "            'f1_weighted': make_scorer(f1_score, average='weighted')\n",
                "        }\n",
                "        \n",
                "        for model_name, model in models.items():\n",
                "            # Perform 5x2 CV\n",
                "            cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
                "            \n",
                "            # Store all 10 results for each fold/repeat\n",
                "            for i in range(10):\n",
                "                row = {\n",
                "                    'imputation': imputation_method,\n",
                "                    'synthesis': synthesis_method,\n",
                "                    'model': model_name,\n",
                "                    'fold_id': i,\n",
                "                    'balanced_accuracy': cv_results['test_balanced_accuracy'][i],\n",
                "                    'accuracy': cv_results['test_accuracy'][i],\n",
                "                    'f1_weighted': cv_results['test_f1_weighted'][i]\n",
                "                }\n",
                "                all_results.append(row)\n",
                "\n",
                "results_df = pd.DataFrame(all_results)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Results Visualization (Distributions)\n",
                "\n",
                "We use boxplots to show the distribution of performance across the 10 folds for each configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(14, 8))\n",
                "sns.boxplot(data=results_df, x='imputation', y='balanced_accuracy', hue='model')\n",
                "plt.title('Balanced Accuracy Distribution by Imputation Method (5x2 CV)')\n",
                "plt.ylabel('Balanced Accuracy')\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(14, 8))\n",
                "sns.boxplot(data=results_df, x='synthesis', y='balanced_accuracy', hue='model')\n",
                "plt.title('Balanced Accuracy Distribution by Synthesis Method (5x2 CV)')\n",
                "plt.ylabel('Balanced Accuracy')\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Statistical Summary and Best Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Group by config and calculate mean/std\n",
                "summary_df = results_df.groupby(['imputation', 'synthesis', 'model']).agg({\n",
                "    'balanced_accuracy': ['mean', 'std'],\n",
                "    'accuracy': 'mean',\n",
                "    'f1_weighted': 'mean'\n",
                "}).reset_index()\n",
                "\n",
                "# Flatten multi-index columns\n",
                "summary_df.columns = ['imputation', 'synthesis', 'model', 'mean_balanced_accuracy', 'std_balanced_accuracy', 'mean_accuracy', 'mean_f1_weighted']\n",
                "\n",
                "best_row = summary_df.sort_values(by='mean_balanced_accuracy', ascending=False).iloc[0]\n",
                "print(\"Best Performing Configuration (Based on Mean Balanced Accuracy):\")\n",
                "print(best_row)\n",
                "\n",
                "print(\"\\nTop 5 Overall:\")\n",
                "display(summary_df.sort_values(by='mean_balanced_accuracy', ascending=False).head(5))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Heatmap (Stacking Model)\n",
                "\n",
                "Visualizing the interaction between imputation and synthesis for the Stacking model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stacking_results = summary_df[summary_df['model'] == 'Stacking']\n",
                "matrix = stacking_results.pivot(index='imputation', columns='synthesis', values='mean_balanced_accuracy')\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.heatmap(matrix, annot=True, cmap='YlGnBu', fmt='.3f')\n",
                "plt.title('Stacking Model - Mean Balanced Accuracy (5x2 CV)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Best Configuration: Confusion Matrix\n",
                "\n",
                "Generating an aggregated confusion matrix for the best overall configuration found above using the manual CV loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_imp = best_row['imputation']\n",
                "best_syn = best_row['synthesis']\n",
                "best_model_name = best_row['model']\n",
                "\n",
                "print(f\"Generating Confusion Matrix for Best Configuration: {best_imp} + {best_syn} with {best_model_name}\")\n",
                "\n",
                "# Load the best dataset\n",
                "best_file_path = f\"../data/synthetic/{best_imp}/uc_diagnostics_{best_syn}.csv\"\n",
                "df_best = pd.read_csv(best_file_path)\n",
                "X_best = df_best.drop(columns=['mayo'])\n",
                "y_best = df_best['mayo']\n",
                "\n",
                "# Define the best model type\n",
                "if best_model_name == 'RF':\n",
                "    model_best = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "elif best_model_name == 'CatBoost':\n",
                "    model_best = CatBoostClassifier(iterations=500, random_seed=42, verbose=0)\n",
                "else: # Stacking\n",
                "    model_best = StackingClassifier(\n",
                "        estimators=[\n",
                "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "            ('cb', CatBoostClassifier(iterations=500, random_seed=42, verbose=0))\n",
                "        ], \n",
                "        final_estimator=LogisticRegression()\n",
                "    )\n",
                "\n",
                "# Manual aggregation over 5x2 folds since cross_val_predict doesn't support repeats\n",
                "labels = np.unique(y_best)\n",
                "cm_total = np.zeros((len(labels), len(labels)), dtype=int)\n",
                "\n",
                "for train_index, test_index in cv.split(X_best, y_best):\n",
                "    X_train_fold, X_test_fold = X_best.iloc[train_index], X_best.iloc[test_index]\n",
                "    y_train_fold, y_test_fold = y_best.iloc[train_index], y_best.iloc[test_index]\n",
                "    \n",
                "    model_best.fit(X_train_fold, y_train_fold)\n",
                "    y_pred_fold = model_best.predict(X_test_fold)\n",
                "    \n",
                "    cm_total += confusion_matrix(y_test_fold, y_pred_fold, labels=labels)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm_total, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=labels, yticklabels=labels)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title(f'Confusion Matrix (Aggregated 5x2 CV)\\n{best_model_name} on {best_imp}+{best_syn}')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}