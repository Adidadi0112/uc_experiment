{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.experimental import enable_iterative_imputer  \n",
                "from sklearn.impute import IterativeImputer\n",
                "from sklearn.impute import KNNImputer\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from fancyimpute import SoftImpute\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import os\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original data shape: (252, 56)\n",
                        "Missing values before imputation: 3939\n"
                    ]
                }
            ],
            "source": [
                "# Load raw data\n",
                "input_path = '../data/raw/uc_diagnostic_tests.csv'\n",
                "df = pd.read_csv(input_path, decimal=',')\n",
                "\n",
                "print(\"Original data shape:\", df.shape)\n",
                "print(\"Missing values before imputation:\", df.isna().sum().sum())\n",
                "\n",
                "# Select numeric columns\n",
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "\n",
                "# Ensure output directory exists\n",
                "os.makedirs('../data/processed', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "mice_imputation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running MICE imputation...\n",
                        "MICE saved to: ../data/processed/uc_diagnostic_tests_mice.csv\n"
                    ]
                }
            ],
            "source": [
                "### 1. MICE Imputation\n",
                "print(\"Running MICE imputation...\")\n",
                "mice_imputer = IterativeImputer(max_iter=20, random_state=42)\n",
                "df_mice = df.copy()\n",
                "df_mice[numeric_cols] = mice_imputer.fit_transform(df[numeric_cols])\n",
                "\n",
                "output_path_mice = '../data/processed/uc_diagnostic_tests_mice.csv'\n",
                "df_mice.to_csv(output_path_mice, index=False)\n",
                "print(f\"MICE saved to: {output_path_mice}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "knn_imputation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running KNN imputation...\n",
                        "KNN saved to: ../data/processed/uc_diagnostic_tests_knn.csv\n"
                    ]
                }
            ],
            "source": [
                "### 2. KNN Imputation\n",
                "print(\"Running KNN imputation...\")\n",
                "knn_imputer = KNNImputer(n_neighbors=5)\n",
                "df_knn = df.copy()\n",
                "df_knn[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])\n",
                "\n",
                "output_path_knn = '../data/processed/uc_diagnostic_tests_knn.csv'\n",
                "df_knn.to_csv(output_path_knn, index=False)\n",
                "print(f\"KNN saved to: {output_path_knn}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "soft_imputation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running SoftImputer...\n",
                        "SoftImpute saved to: ../data/processed/uc_diagnostic_tests_softimpute.csv\n"
                    ]
                }
            ],
            "source": [
                "### 3. SoftImpute\n",
                "print(\"Running SoftImputer...\")\n",
                "soft_imputer = SoftImpute(verbose=False)\n",
                "df_soft = df.copy()\n",
                "# SoftImpute usually requires matrix, returns matrix\n",
                "df_soft[numeric_cols] = soft_imputer.fit_transform(df[numeric_cols].values)\n",
                "\n",
                "output_path_soft = '../data/processed/uc_diagnostic_tests_softimpute.csv'\n",
                "df_soft.to_csv(output_path_soft, index=False)\n",
                "print(f\"SoftImpute saved to: {output_path_soft}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "gain_function",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running GAIN imputation...\n",
                        "GAIN saved to: ../data/processed/uc_diagnostic_tests_gain.csv\n"
                    ]
                }
            ],
            "source": [
                "### 4. GAIN Imputation\n",
                "\n",
                "def gain_imputation(data_x, gain_parameters):\n",
                "    # Define parameters\n",
                "    batch_size = gain_parameters['batch_size']\n",
                "    hint_rate = gain_parameters['hint_rate']\n",
                "    alpha = gain_parameters['alpha']\n",
                "    iterations = gain_parameters['iterations']\n",
                "    \n",
                "    No = len(data_x)\n",
                "    Dim = len(data_x[0])\n",
                "    \n",
                "    # Define Mask\n",
                "    data_m = 1 - np.isnan(data_x)\n",
                "    \n",
                "    # Normalization (GAIN needs normalized inputs 0-1 usually, but we need to inverse it back)\n",
                "    min_val = np.zeros(Dim)\n",
                "    max_val = np.zeros(Dim)\n",
                "    \n",
                "    # Creating a copy to avoid modifying original dataframe in place if passed directly\n",
                "    norm_data = data_x.copy()\n",
                "    \n",
                "    for i in range(Dim):\n",
                "        min_val[i] = np.nanmin(norm_data[:,i])\n",
                "        norm_data[:,i] = norm_data[:,i] - min_val[i]\n",
                "        max_val[i] = np.nanmax(norm_data[:,i])\n",
                "        norm_data[:,i] = norm_data[:,i] / (max_val[i] + 1e-6)\n",
                "        \n",
                "    # Renormalization helper\n",
                "    def renormalization(norm_data, min_val, max_val):\n",
                "        renorm_data = norm_data.copy()\n",
                "        for i in range(Dim):\n",
                "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)\n",
                "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
                "        return renorm_data\n",
                "        \n",
                "    # Fill NaN with 0 for training\n",
                "    norm_data = np.nan_to_num(norm_data, nan=0)\n",
                "    \n",
                "    # Convert to torch tensors\n",
                "    data_x_tensor = torch.tensor(norm_data, dtype=torch.float32)\n",
                "    data_m_tensor = torch.tensor(data_m, dtype=torch.float32)\n",
                "    \n",
                "    # Generator\n",
                "    class Generator(nn.Module):\n",
                "        def __init__(self, dim):\n",
                "            super().__init__()\n",
                "            self.fc1 = nn.Linear(dim * 2, dim)\n",
                "            self.fc2 = nn.Linear(dim, dim)\n",
                "            self.fc3 = nn.Linear(dim, dim)\n",
                "            self.relu = nn.ReLU()\n",
                "            self.sigmoid = nn.Sigmoid()\n",
                "            \n",
                "        def forward(self, x, m):\n",
                "            inputs = torch.cat([x, m], dim=1)\n",
                "            h1 = self.relu(self.fc1(inputs))\n",
                "            h2 = self.relu(self.fc2(h1))\n",
                "            out = self.sigmoid(self.fc3(h2))\n",
                "            return out\n",
                "            \n",
                "    # Discriminator\n",
                "    class Discriminator(nn.Module):\n",
                "        def __init__(self, dim):\n",
                "            super().__init__()\n",
                "            self.fc1 = nn.Linear(dim * 2, dim)\n",
                "            self.fc2 = nn.Linear(dim, dim)\n",
                "            self.fc3 = nn.Linear(dim, dim)\n",
                "            self.relu = nn.ReLU()\n",
                "            self.sigmoid = nn.Sigmoid()\n",
                "            \n",
                "        def forward(self, x, h):\n",
                "            inputs = torch.cat([x, h], dim=1)\n",
                "            d1 = self.relu(self.fc1(inputs))\n",
                "            d2 = self.relu(self.fc2(d1))\n",
                "            out = self.sigmoid(self.fc3(d2))\n",
                "            return out\n",
                "\n",
                "    # Initialize models\n",
                "    generator = Generator(Dim)\n",
                "    discriminator = Discriminator(Dim)\n",
                "    \n",
                "    optimizer_G = optim.Adam(generator.parameters())\n",
                "    optimizer_D = optim.Adam(discriminator.parameters())\n",
                "    \n",
                "    # Training\n",
                "    for it in range(iterations):\n",
                "        # Sample batch\n",
                "        idx = np.random.permutation(No)[:batch_size]\n",
                "        X_mb = data_x_tensor[idx]\n",
                "        M_mb = data_m_tensor[idx]\n",
                "        \n",
                "        # Random noise for Missing values\n",
                "        Z_mb = torch.rand((batch_size, Dim)) * 0.01\n",
                "        \n",
                "        # Hint vector\n",
                "        H_mb_temp = torch.rand((batch_size, Dim))\n",
                "        H_mb = (H_mb_temp > (1 - hint_rate)).float()\n",
                "        H_mb = M_mb * H_mb + 0.5 * (1 - H_mb)\n",
                "        \n",
                "        # Combine random noise with data\n",
                "        X_mb_noise = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
                "        \n",
                "        # Train Discriminator\n",
                "        optimizer_D.zero_grad()\n",
                "        G_sample = generator(X_mb_noise, M_mb)\n",
                "        Hat_X = M_mb * X_mb + (1 - M_mb) * G_sample\n",
                "        D_prob = discriminator(Hat_X, H_mb)\n",
                "        \n",
                "        D_loss = -torch.mean(M_mb * torch.log(D_prob + 1e-8) + (1 - M_mb) * torch.log(1 - D_prob + 1e-8))\n",
                "        D_loss.backward()\n",
                "        optimizer_D.step()\n",
                "        \n",
                "        # Train Generator\n",
                "        optimizer_G.zero_grad()\n",
                "        G_sample = generator(X_mb_noise, M_mb)\n",
                "        Hat_X = M_mb * X_mb + (1 - M_mb) * G_sample\n",
                "        D_prob = discriminator(Hat_X, H_mb)\n",
                "        \n",
                "        G_loss_temp = -torch.mean((1 - M_mb) * torch.log(D_prob + 1e-8))\n",
                "        MSE_loss = torch.mean((M_mb * X_mb - M_mb * G_sample)**2) / torch.mean(M_mb)\n",
                "        \n",
                "        G_loss = G_loss_temp + alpha * MSE_loss\n",
                "        G_loss.backward()\n",
                "        optimizer_G.step()\n",
                "        \n",
                "    # Final Imputation\n",
                "    Z_mb_final = torch.rand((No, Dim)) * 0.01\n",
                "    X_mb_final = data_m_tensor * data_x_tensor + (1 - data_m_tensor) * Z_mb_final\n",
                "    \n",
                "    generated_data = generator(X_mb_final, data_m_tensor).detach().numpy()\n",
                "    \n",
                "    # Re-normalize back to original range\n",
                "    imputed_data_norm = data_m * norm_data + (1 - data_m) * generated_data\n",
                "    imputed_data = renormalization(imputed_data_norm, min_val, max_val)\n",
                "    \n",
                "    return imputed_data\n",
                "\n",
                "# GAIN parameters\n",
                "gain_parameters = {\n",
                "    'batch_size': 32,\n",
                "    'hint_rate': 0.9,\n",
                "    'alpha': 100,\n",
                "    'iterations': 5000\n",
                "}\n",
                "\n",
                "print(\"Running GAIN imputation...\")\n",
                "imputed_data_gain = gain_imputation(df[numeric_cols].values, gain_parameters)\n",
                "\n",
                "df_gain = df.copy()\n",
                "df_gain[numeric_cols] = imputed_data_gain\n",
                "\n",
                "output_path_gain = '../data/processed/uc_diagnostic_tests_gain.csv'\n",
                "df_gain.to_csv(output_path_gain, index=False)\n",
                "print(f\"GAIN saved to: {output_path_gain}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running PMM imputation...\n",
                        "PMM saved to: ../data/processed/uc_diagnostic_tests_pmm.csv\n"
                    ]
                }
            ],
            "source": [
                "### 5. PMM Imputation (Predictive Mean Matching)\n",
                "print(\"Running PMM imputation...\")\n",
                "\n",
                "def pmm_imputation(df, numeric_cols):\n",
                "    df_pmm = df.copy()\n",
                "    df_temp = df_pmm[numeric_cols].fillna(df_pmm[numeric_cols].mean())\n",
                "    \n",
                "    for col in numeric_cols:\n",
                "        if df[col].isna().sum() == 0:\n",
                "            continue\n",
                "            \n",
                "        is_nan = df[col].isna()\n",
                "        X = df_temp.drop(columns=[col])\n",
                "        y = df[col]\n",
                "        \n",
                "        X_obs, y_obs = X[~is_nan], y[~is_nan]\n",
                "        X_mis = X[is_nan]\n",
                "        \n",
                "        model = LinearRegression()\n",
                "        model.fit(X_obs, y_obs)\n",
                "        \n",
                "        pred_obs = model.predict(X_obs).reshape(-1, 1)\n",
                "        pred_mis = model.predict(X_mis).reshape(-1, 1)\n",
                "        \n",
                "        nn = NearestNeighbors(n_neighbors=1)\n",
                "        nn.fit(pred_obs)\n",
                "        _, indices = nn.kneighbors(pred_mis)\n",
                "        \n",
                "        imputed_values = y_obs.iloc[indices.flatten()].values\n",
                "        df_pmm.loc[is_nan, col] = imputed_values\n",
                "        \n",
                "    return df_pmm\n",
                "\n",
                "# Run PMM\n",
                "df_pmm = pmm_imputation(df, numeric_cols)\n",
                "\n",
                "# Save result\n",
                "output_path_pmm = '../data/processed/uc_diagnostic_tests_pmm.csv'\n",
                "df_pmm.to_csv(output_path_pmm, index=False)\n",
                "print(f\"PMM saved to: {output_path_pmm}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "result_check",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "All imputations complete.\n",
                        "MICE missing: 0\n",
                        "KNN missing: 0\n",
                        "SoftImpute missing: 0\n",
                        "GAIN missing: 0\n",
                        "PMM missing: 0\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nAll imputations complete.\")\n",
                "print(f\"MICE missing: {df_mice.isna().sum().sum()}\")\n",
                "print(f\"KNN missing: {df_knn.isna().sum().sum()}\")\n",
                "print(f\"SoftImpute missing: {df_soft.isna().sum().sum()}\")\n",
                "print(f\"GAIN missing: {df_gain.isna().sum().sum()}\")\n",
                "print(f\"PMM missing: {df_pmm.isna().sum().sum()}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "uc_experiment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
